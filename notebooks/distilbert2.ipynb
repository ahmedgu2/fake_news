{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twitter fake news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>se acuerdan de la pel cula el despu de ana rec...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>milenagimon miren sandi en ny tremenda imagen ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>buena la foto del hurac n sandi recuerda la pe...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>scari shit hurrican ny http co e jlbufh</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>fave place world nyc hurrican sandi statueofli...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0             tweetId  \\\n",
       "0             0           0  263046056240115712   \n",
       "1             1           1  262995061304852481   \n",
       "2             2           2  262979898002534400   \n",
       "3             3           3  262996108400271360   \n",
       "4             4           4  263018881839411200   \n",
       "\n",
       "                                           tweetText label  \n",
       "0  se acuerdan de la pel cula el despu de ana rec...  fake  \n",
       "1  milenagimon miren sandi en ny tremenda imagen ...  fake  \n",
       "2  buena la foto del hurac n sandi recuerda la pe...  fake  \n",
       "3            scari shit hurrican ny http co e jlbufh  fake  \n",
       "4  fave place world nyc hurrican sandi statueofli...  fake  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../data/raw/twitter/tweetstrain2015.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen rt shyman eclips iss http co je hcfpvfn</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>absolut beauti rt shyman eclips iss http co oq...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>shyman eclips iss http co c vfboscrj wow amaz</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>eclips iss http co en otvsu</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>ebonfigli clips vue de l iss autr chose http c...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0             tweetId  \\\n",
       "0             0           0  578854927457349632   \n",
       "1             1           1  578874632670953472   \n",
       "2             2           2  578891261353984000   \n",
       "3             3           3  578846612312748032   \n",
       "4             4           4  578975333841551360   \n",
       "\n",
       "                                           tweetText label  \n",
       "0    kereeen rt shyman eclips iss http co je hcfpvfn  fake  \n",
       "1  absolut beauti rt shyman eclips iss http co oq...  fake  \n",
       "2      shyman eclips iss http co c vfboscrj wow amaz  fake  \n",
       "3                        eclips iss http co en otvsu  fake  \n",
       "4  ebonfigli clips vue de l iss autr chose http c...  fake  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('../data/raw/twitter/tweetstest2015.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode output\n",
    "label_mapping = {'fake': 1, 'real': 0}\n",
    "train.label = train.label.map(label_mapping)\n",
    "test.label = test.label.map(label_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()\n",
    "X_train, X_val, y_train, y_val = train_test_split(train['tweetText'], train['label'], test_size=0.2, random_state=42)\n",
    "X_test, y_test = test['tweetText'], test.label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = tokenizer(\n",
    "    list(X_train.values),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "X_val_encoded = tokenizer(\n",
    "    list(X_val.values),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "X_test_encoded = tokenizer(\n",
    "    list(X_test),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FakeNewsDataset(X_train_encoded, y_train.values)\n",
    "val_dataset = FakeNewsDataset(X_val_encoded, y_val.values)\n",
    "test_dataset = FakeNewsDataset(X_test_encoded, y_test.values)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['HF_MLFLOW_LOG_ARTIFACTS'] = \"1\" # save models as artifact for the expirment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/twitter_dataset_models/distilbert/',          # output directory\n",
    "    num_train_epochs=5,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=400,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=400,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    save_steps=400\n",
    "\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,             # evaluation dataset\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\msi\\anaconda3\\envs\\nlp_project\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 11421\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4284\n",
      "  Number of trainable parameters = 66955010\n",
      "  0%|          | 0/4284 [00:00<?, ?it/s]C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_32636\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 12%|â–ˆâ–        | 500/4284 [02:35<19:41,  3.20it/s]***** Running Evaluation *****\n",
      "  Num examples = 2856\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5091, 'learning_rate': 5e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 12%|â–ˆâ–        | 500/4284 [03:03<19:41,  3.20it/s]Saving model checkpoint to ../models\\checkpoint-500\n",
      "Configuration saved in ../models\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3523949384689331, 'eval_accuracy': 0.8662464985994398, 'eval_f1': 0.9028978139298424, 'eval_precision': 0.8538461538461538, 'eval_recall': 0.9579288025889967, 'eval_runtime': 28.5341, 'eval_samples_per_second': 100.091, 'eval_steps_per_second': 12.511, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../models\\checkpoint-500\\pytorch_model.bin\n",
      "C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_32636\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 1000/4284 [06:00<21:14,  2.58it/s] ***** Running Evaluation *****\n",
      "  Num examples = 2856\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3571, 'learning_rate': 4.339323467230444e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 23%|â–ˆâ–ˆâ–Ž       | 1000/4284 [06:32<21:14,  2.58it/s]Saving model checkpoint to ../models\\checkpoint-1000\n",
      "Configuration saved in ../models\\checkpoint-1000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3548908531665802, 'eval_accuracy': 0.882703081232493, 'eval_f1': 0.9104995992519369, 'eval_precision': 0.9020645844362096, 'eval_recall': 0.919093851132686, 'eval_runtime': 31.9964, 'eval_samples_per_second': 89.26, 'eval_steps_per_second': 11.157, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../models\\checkpoint-1000\\pytorch_model.bin\n",
      "C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_32636\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1500/4284 [09:40<17:36,  2.64it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 2856\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3033, 'learning_rate': 3.678646934460888e-05, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 1500/4284 [10:12<17:36,  2.64it/s]Saving model checkpoint to ../models\\checkpoint-1500\n",
      "Configuration saved in ../models\\checkpoint-1500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32016709446907043, 'eval_accuracy': 0.8998599439775911, 'eval_f1': 0.9270780214176441, 'eval_precision': 0.879110251450677, 'eval_recall': 0.9805825242718447, 'eval_runtime': 32.3245, 'eval_samples_per_second': 88.354, 'eval_steps_per_second': 11.044, 'epoch': 1.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../models\\checkpoint-1500\\pytorch_model.bin\n",
      "C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_32636\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2000/4284 [13:21<14:00,  2.72it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 2856\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2608, 'learning_rate': 3.017970401691332e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 2000/4284 [13:53<14:00,  2.72it/s]Saving model checkpoint to ../models\\checkpoint-2000\n",
      "Configuration saved in ../models\\checkpoint-2000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3136407434940338, 'eval_accuracy': 0.898109243697479, 'eval_f1': 0.9223793011469726, 'eval_precision': 0.912401055408971, 'eval_recall': 0.9325782092772384, 'eval_runtime': 31.9028, 'eval_samples_per_second': 89.522, 'eval_steps_per_second': 11.19, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../models\\checkpoint-2000\\pytorch_model.bin\n",
      "C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_32636\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2500/4284 [17:05<11:15,  2.64it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 2856\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2584, 'learning_rate': 2.357293868921776e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 2500/4284 [17:37<11:15,  2.64it/s]Saving model checkpoint to ../models\\checkpoint-2500\n",
      "Configuration saved in ../models\\checkpoint-2500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2928997576236725, 'eval_accuracy': 0.9117647058823529, 'eval_f1': 0.9337539432176656, 'eval_precision': 0.9107692307692308, 'eval_recall': 0.9579288025889967, 'eval_runtime': 31.8895, 'eval_samples_per_second': 89.559, 'eval_steps_per_second': 11.195, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../models\\checkpoint-2500\\pytorch_model.bin\n",
      "C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_32636\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3000/4284 [20:48<08:13,  2.60it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 2856\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2239, 'learning_rate': 1.69661733615222e-05, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 3000/4284 [21:20<08:13,  2.60it/s]Saving model checkpoint to ../models\\checkpoint-3000\n",
      "Configuration saved in ../models\\checkpoint-3000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4574283957481384, 'eval_accuracy': 0.8991596638655462, 'eval_f1': 0.9215258855585832, 'eval_precision': 0.9311674008810573, 'eval_recall': 0.9120819848975189, 'eval_runtime': 31.9216, 'eval_samples_per_second': 89.469, 'eval_steps_per_second': 11.184, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../models\\checkpoint-3000\\pytorch_model.bin\n",
      "C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_32636\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3500/4284 [24:27<04:41,  2.78it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 2856\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1693, 'learning_rate': 1.0359408033826638e-05, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 3500/4284 [24:59<04:41,  2.78it/s]Saving model checkpoint to ../models\\checkpoint-3500\n",
      "Configuration saved in ../models\\checkpoint-3500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3657529950141907, 'eval_accuracy': 0.9110644257703081, 'eval_f1': 0.9327330508474575, 'eval_precision': 0.9162330905306972, 'eval_recall': 0.9498381877022654, 'eval_runtime': 31.8117, 'eval_samples_per_second': 89.778, 'eval_steps_per_second': 11.222, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../models\\checkpoint-3500\\pytorch_model.bin\n",
      "C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_32636\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4000/4284 [28:09<01:48,  2.63it/s]  ***** Running Evaluation *****\n",
      "  Num examples = 2856\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1665, 'learning_rate': 3.7526427061310788e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 4000/4284 [28:42<01:48,  2.63it/s]Saving model checkpoint to ../models\\checkpoint-4000\n",
      "Configuration saved in ../models\\checkpoint-4000\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39100727438926697, 'eval_accuracy': 0.9093137254901961, 'eval_f1': 0.930914910642838, 'eval_precision': 0.920844327176781, 'eval_recall': 0.9412081984897519, 'eval_runtime': 32.3478, 'eval_samples_per_second': 88.29, 'eval_steps_per_second': 11.036, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../models\\checkpoint-4000\\pytorch_model.bin\n",
      "C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_32636\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4284/4284 [30:36<00:00,  2.87it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4284/4284 [30:36<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1836.8729, 'train_samples_per_second': 18.653, 'train_steps_per_second': 2.332, 'train_loss': 0.2736295999265185, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4284, training_loss=0.2736295999265185, metrics={'train_runtime': 1836.8729, 'train_samples_per_second': 18.653, 'train_steps_per_second': 2.332, 'train_loss': 0.2736295999265185, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('../models/twitter_dataset_models/checkpoint-4000/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/118 [00:00<?, ?it/s]C:\\Users\\msi\\AppData\\Local\\Temp\\ipykernel_13440\\2842400840.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 118/118 [00:35<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "from tqdm import tqdm\n",
    "acc = 0.0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader):\n",
    "        input_ids, labels = data['input_ids'].to('cuda'), data['labels'].to('cuda')\n",
    "        out = torch.softmax(model(input_ids).logits, dim=1)\n",
    "        acc += torch.sum(torch.argmax(out, dim=1) == labels) / len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8676087856292725\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {acc / len(test_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7481d9fdbdb5bc3cfa1c41a158e56f79c2803b01a0eeac01b32455c6569565e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
